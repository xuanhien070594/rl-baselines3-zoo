# Tuned
MountainCarContinuous-v0:
  n_timesteps: 300000
  policy: 'MlpPolicy'
  noise_type: 'ornstein-uhlenbeck'
  noise_std: 0.5

pendulum-v0:
  n_timesteps: 20000
  policy: 'mlppolicy'
  gamma: 0.98
  buffer_size: 200000
  learning_starts: 10000
  noise_type: 'normal'
  noise_std: 0.1
  gradient_steps: -1
  train_freq: [1, "episode"]
  learning_rate: !!float 1e-3
  policy_kwargs: "dict(net_arch=[400, 300])"

LunarLanderContinuous-v2:
  n_timesteps: !!float 3e5
  policy: 'MlpPolicy'
  gamma: 0.98
  buffer_size: 200000
  learning_starts: 10000
  noise_type: 'normal'
  noise_std: 0.1
  gradient_steps: -1
  train_freq: [1, "episode"]
  learning_rate: !!float 1e-3
  policy_kwargs: "dict(net_arch=[400, 300])"

BipedalWalker-v3:
  n_timesteps: !!float 1e6
  policy: 'MlpPolicy'
  gamma: 0.98
  buffer_size: 200000
  learning_starts: 10000
  noise_type: 'normal'
  noise_std: 0.1
  gradient_steps: -1
  train_freq: [1, "episode"]
  learning_rate: !!float 1e-3
  policy_kwargs: "dict(net_arch=[400, 300])"

# To be tuned
BipedalWalkerHardcore-v3:
  n_timesteps: !!float 1e7
  policy: 'MlpPolicy'
  gamma: 0.99
  buffer_size: 1000000
  learning_starts: 10000
  noise_type: 'normal'
  noise_std: 0.1
  batch_size: 256
  train_freq: 1
  learning_rate: lin_7e-4
  policy_kwargs: "dict(net_arch=[400, 300])"

# Tuned
HalfCheetahBulletEnv-v0: &pybullet-defaults
  n_timesteps: !!float 1e6
  policy: 'MlpPolicy'
  gamma: 0.98
  buffer_size: 200000
  learning_starts: 10000
  noise_type: 'normal'
  noise_std: 0.1
  gradient_steps: -1
  train_freq: [1, "episode"]
  learning_rate: !!float 1e-3
  policy_kwargs: "dict(net_arch=[400, 300])"

AntBulletEnv-v0:
  <<: *pybullet-defaults

HopperBulletEnv-v0:
  <<: *pybullet-defaults

Walker2DBulletEnv-v0:
  <<: *pybullet-defaults


# TO BE tested
HumanoidBulletEnv-v0:
  n_timesteps: !!float 2e6
  policy: 'MlpPolicy'
  gamma: 0.98
  buffer_size: 200000
  learning_starts: 10000
  noise_type: 'normal'
  noise_std: 0.1
  train_freq: 1
  learning_rate: !!float 1e-3
  policy_kwargs: "dict(net_arch=[400, 300])"

# Tuned
ReacherBulletEnv-v0:
  <<: *pybullet-defaults
  n_timesteps: !!float 3e5

# Tuned
InvertedDoublePendulumBulletEnv-v0:
  <<: *pybullet-defaults

# Tuned
InvertedPendulumSwingupBulletEnv-v0:
  <<: *pybullet-defaults
  n_timesteps: !!float 3e5


MinitaurBulletEnv-v0:
  n_timesteps: !!float 1e6
  policy: 'MlpPolicy'
  gamma: 0.99
  buffer_size: 1000000
  noise_type: 'normal'
  noise_std: 0.1
  learning_starts: 10000
  batch_size: 100
  learning_rate: !!float 1e-3
  train_freq: 1
  gradient_steps: 1
  policy_kwargs: "dict(net_arch=[400, 300])"

# === Mujoco Envs ===

HalfCheetah-v3: &mujoco-defaults
  n_timesteps: !!float 1e6
  policy: 'MlpPolicy'
  learning_starts: 10000
  noise_type: 'normal'
  noise_std: 0.1

Ant-v3:
  <<: *mujoco-defaults

Hopper-v3:
  <<: *mujoco-defaults
  # SAC Hyperparams
  train_freq: 1
  gradient_steps: 1
  learning_rate: !!float 3e-4
  batch_size: 256

Walker2d-v3:
  <<: *mujoco-defaults

Humanoid-v3:
  <<: *mujoco-defaults
  n_timesteps: !!float 2e6
  # SAC Hyperparams
  train_freq: 1
  gradient_steps: 1
  learning_rate: !!float 3e-4
  batch_size: 256

# Tuned
Swimmer-v3:
  <<: *mujoco-defaults
  gamma: 0.9999
  train_freq: 1
  gradient_steps: 1

# === Custom Envs ===

# Tuned 01/31/2022
CartPole-Softwalls-v1:
  n_timesteps: !!float 5e5
  policy: 'MlpPolicy'
  gamma: 0.999
  buffer_size: 1000000
  learning_starts: 0
  gradient_steps: 64
  batch_size: 1024
  noise_type: 'normal'
  noise_std: 0.6990451822
  tau: 0.08
  train_freq: 64
  learning_rate: !!float 0.001467701779
  policy_kwargs: "dict(net_arch=[64, 64])"

# Tuned 01/31/2022
CartPole-Softwalls-v0:
  n_timesteps: !!float 5e5
  policy: 'MlpPolicy'
  gamma: 0.995
  buffer_size: 1000000
  learning_starts: 0
  gradient_steps: 4
  batch_size: 1024
  noise_type: 'normal'
  noise_std: 0.7105764619
  tau: 0.02
  train_freq: 4
  learning_rate: !!float 0.0006927145033
  policy_kwargs: "dict(net_arch=[256, 256])"

CartPole-continuous-input-v0:
  n_timesteps: !!float 5e5
  policy: 'MlpPolicy'
  gamma: 0.999
  buffer_size: 100000
  learning_starts: 10000
  noise_type: 'normal'
  noise_std: 0.3502860017001225
  batch_size: 256
  tau: 0.05
  train_freq: 128
  learning_rate: !!float 0.0008949771423406326
  policy_kwargs: "dict(net_arch=[400, 300])"

Box-Pushing-with-Arm-v0:
  n_timesteps: !!float 1e6
  policy: 'MlpPolicy'
  gamma: 0.999
  buffer_size: 100000
  learning_starts: 10000
  noise_type: 'normal'
  noise_std: 0.3502860017001225
  batch_size: 256
  tau: 0.05
  train_freq: 128
  learning_rate: !!float 0.0008949771423406326
  policy_kwargs: "dict(net_arch=[400, 300])"

# Inverted-Pendulum-Softwall-v1:
#   n_timesteps: !!float 5e5
#   policy: 'MlpPolicy'
#   gamma: 0.999
#   buffer_size: 1000000
#   learning_starts: 0
#   gradient_steps: 64
#   batch_size: 1024
#   noise_type: 'normal'
#   noise_std: 0.6990451822
#   tau: 0.08
#   train_freq: 64
#   learning_rate: !!float 0.001467701779
#   policy_kwargs: "dict(net_arch=[64, 64])"

Inverted-Pendulum-Softwall-v1:
  n_timesteps: !!float 5e5
  policy: 'MlpPolicy'
  gamma: 0.98
  buffer_size: 10000
  learning_starts: 0
  gradient_steps: 1
  batch_size: 2048
  noise_type: 'normal'
  noise_std: 0.9023963531714942
  tau: 0.01
  train_freq: 1
  learning_rate: !!float 0.0029876718764720728
  policy_kwargs: "dict(net_arch=[256, 256])"

Inverted-Pendulum-Softwall-v0:
  n_timesteps: !!float 5e5
  policy: 'MlpPolicy'
  gamma: 0.995
  buffer_size: 1000000
  learning_starts: 0
  gradient_steps: 4
  batch_size: 1024
  noise_type: 'normal'
  noise_std: 0.7105764619
  tau: 0.02
  train_freq: 4
  learning_rate: !!float 0.0006927145033
  policy_kwargs: "dict(net_arch=[256, 256])"
